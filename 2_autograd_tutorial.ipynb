{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMhzkXYGfbvVlyDq1bkDiBQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MichaelYeung2004/pytorch_tytorials/blob/main/2_autograd_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#torch.autograd 入门教程"
      ],
      "metadata": {
        "id": "ZjtWQwnfCbqB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`torch.autograd` 是 PyTorch 的自动微分引擎，为神经网络训练提供动力。在本节中，你将从概念上理解 autograd 是如何帮助神经网络进行训练的。"
      ],
      "metadata": {
        "id": "iLJWSVbXDacY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##背景\n",
        "神经网络 (NNs) 是作用于某些输入数据上的一系列嵌套函数。这些函数由参数（包括权重和偏置）定义，在 PyTorch 中，这些参数存储在张量（tensor）中。\n",
        "\n",
        "训练一个神经网络分为两个步骤：\n",
        "\n",
        "**前向传播 (Forward Propagation):** 在前向传播中，神经网络对其认为正确的输出做出最佳猜测。它让输入数据流经每一个函数来做出这个猜测。\n",
        "\n",
        "**反向传播 (Backward Propagation):** 在反向传播中，神经网络根据其猜测的误差成比例地调整其参数。它通过从输出开始反向遍历，收集误差相对于函数参数的导数（即**梯度**），并使用梯度下降来优化参数。想更深入地了解反向传播，请观看 [3Blue1Brown](https://www.google.com/url?sa=E&q=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DIlg3gGewQ5U) 的这个视频。"
      ],
      "metadata": {
        "id": "b_TFWembDewP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##在 PyTorch 中的使用\n",
        "我们来看一个单步训练的例子。在这个例子中，我们从 torchvision 加载一个预训练的 resnet18 模型。我们创建一个随机数据张量来表示一个具有 3 个通道、高和宽均为 64 的单张图片，以及其对应的标签（初始化为一些随机值）。在预训练模型中，标签的形状为 (1, 1000)。"
      ],
      "metadata": {
        "id": "psBoJ2qiFEjf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">注意\n",
        "本教程仅在 CPU 上运行，无法在 GPU 设备上工作（即使张量被移动到 CUDA）。"
      ],
      "metadata": {
        "id": "12-sS7DxJ1xD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CCBqsI8DBbrN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e3d8168-b435-4e8e-f4b7-a760e5e2d384"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 84.9MB/s]\n"
          ]
        }
      ],
      "source": [
        "import torch # 导入torch库\n",
        "from torchvision.models import resnet18, ResNet18_Weights # 导入resnet18模型\n",
        "model = resnet18(weights=ResNet18_Weights.DEFAULT) # 加载resnet18模型\n",
        "data = torch.rand(1, 3, 64, 64) # 创建一个1x3x64x64的张量\n",
        "labels = torch.rand(1, 1000) # 创建一个1x1000的张量"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "接下来，我们让输入数据通过模型的每一层，以进行预测。这就是**前向传播**。"
      ],
      "metadata": {
        "id": "4Gu2Q8MWJx5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = model(data) # 前向传播\n",
        "print(prediction) # 打印预测结果"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWx8ynYyJ-0z",
        "outputId": "ce23205b-3042-47bd-d112-d9566959d702"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-4.9530e-01, -7.8792e-01, -6.1362e-01, -1.6742e+00, -7.5752e-01,\n",
            "         -4.7912e-01, -8.4931e-01,  3.9081e-01,  2.3745e-01, -1.0047e+00,\n",
            "         -9.2826e-01, -8.4946e-01, -2.2649e-02, -9.0601e-01, -1.1432e+00,\n",
            "         -3.8299e-01, -7.3267e-01, -2.6334e-01, -4.3612e-01, -3.5710e-01,\n",
            "         -1.5683e+00, -7.1546e-01, -1.7900e+00,  9.8207e-02, -8.8643e-01,\n",
            "         -9.5062e-01, -6.9475e-01, -1.1729e+00, -7.0029e-01, -7.0490e-01,\n",
            "         -6.1555e-01, -7.2706e-01, -2.1651e-01, -5.9308e-01, -4.5647e-01,\n",
            "         -3.3289e-01,  6.0432e-01, -5.7369e-01, -3.2935e-01,  1.5166e-01,\n",
            "         -6.8280e-01, -6.8452e-01, -9.5735e-01, -3.1721e-01, -6.1356e-01,\n",
            "         -3.7029e-01, -7.4383e-01, -1.8931e-01, -1.1455e+00, -1.3509e+00,\n",
            "         -4.6940e-01,  6.3680e-01, -9.0210e-02, -4.7894e-01, -1.8549e-01,\n",
            "         -9.7505e-01, -1.3229e-01, -1.1156e+00, -2.3980e-01, -2.0381e-01,\n",
            "          9.9811e-01,  1.1145e-01, -5.5093e-02,  3.8330e-01, -5.6889e-01,\n",
            "         -9.1272e-02, -1.5755e-01, -2.8038e-01, -5.8237e-01, -1.3457e+00,\n",
            "         -1.9422e+00,  4.8730e-02, -1.5077e+00, -4.8176e-01, -1.3128e+00,\n",
            "         -1.3955e+00,  5.1134e-02, -7.3583e-01,  2.0277e-01, -3.2805e-02,\n",
            "         -7.1562e-01, -1.6502e+00,  1.4588e-01, -7.4599e-01, -5.3530e-01,\n",
            "          2.2373e-01,  3.7015e-01,  3.1445e-01, -2.4088e-01, -7.8605e-01,\n",
            "         -1.0466e+00, -9.9741e-01, -1.9450e+00, -4.0198e-01,  3.2203e-02,\n",
            "         -2.0746e+00, -4.3534e-01, -2.4185e-01, -1.2505e+00, -1.2012e-01,\n",
            "         -1.2885e+00, -1.1370e+00, -8.6726e-01, -2.3050e-01, -1.2542e-01,\n",
            "         -4.1544e-01, -6.2006e-01, -1.5554e+00, -1.1619e+00, -1.5268e+00,\n",
            "         -1.1732e+00, -4.3986e-01,  1.1179e+00,  3.6560e-01,  2.5598e-01,\n",
            "         -1.0097e+00, -6.7768e-01, -5.1880e-01,  5.8570e-01, -3.8215e-01,\n",
            "         -7.2770e-01,  1.2673e-01,  1.2249e-01,  1.5241e-01,  1.0497e+00,\n",
            "         -1.9926e-02,  2.4250e-01, -1.2474e+00, -1.2701e+00, -1.0640e+00,\n",
            "         -1.3339e+00, -1.3088e+00, -1.2498e+00, -1.1314e+00, -3.4242e-01,\n",
            "         -1.3432e+00, -8.9345e-01, -1.3310e+00, -1.3356e+00, -1.3715e+00,\n",
            "         -1.3760e+00, -1.6183e+00, -2.0838e+00, -1.6102e+00, -7.9719e-01,\n",
            "         -5.7088e-01, -1.0859e+00, -2.0384e+00, -1.2512e+00, -1.4170e+00,\n",
            "          2.7078e-01,  1.6258e+00, -1.1082e+00, -5.5700e-01,  4.0227e-03,\n",
            "          2.8435e-01, -2.9630e-01, -2.9764e-01,  2.6386e-01,  2.2943e-01,\n",
            "          5.9933e-01,  9.3573e-01,  4.9481e-01,  8.8066e-01,  5.7454e-01,\n",
            "         -1.9350e-01,  1.4082e-02, -2.0122e-01,  6.1265e-01, -1.2738e-01,\n",
            "         -1.5856e-01,  8.4057e-01,  6.3606e-01,  3.4689e-01,  1.7356e-01,\n",
            "         -4.9641e-01,  4.4786e-01,  1.0147e-01,  8.8418e-01,  6.0721e-01,\n",
            "          5.6276e-01,  6.5649e-02,  7.2333e-01, -2.3567e-03,  5.1350e-01,\n",
            "          8.8845e-01,  6.3453e-01,  3.2079e-01,  4.2693e-01,  7.0509e-01,\n",
            "         -5.7749e-01,  4.7765e-01,  3.8650e-01,  6.5827e-01, -6.0088e-01,\n",
            "          6.1952e-01,  1.3062e-01,  1.0838e-01,  2.7770e-01,  4.5905e-01,\n",
            "          1.9715e-01,  4.7384e-01,  6.4814e-01,  4.0671e-01,  1.3515e-01,\n",
            "          1.8418e-01, -1.7497e-01,  5.5994e-01,  1.1544e+00,  5.3460e-01,\n",
            "          1.5028e-01,  4.7322e-01,  6.6871e-01, -7.4603e-02,  1.1892e-01,\n",
            "          3.7625e-01, -1.2488e-01,  3.3020e-01, -3.2976e-01,  6.9925e-01,\n",
            "          1.7157e-01, -2.1034e-01,  9.8731e-02,  5.7808e-01,  4.6533e-02,\n",
            "          4.9496e-01,  1.8004e-01,  9.7324e-01, -3.1511e-01, -2.0488e-01,\n",
            "         -1.5269e-01,  2.9419e-01,  4.1671e-01, -1.1365e-01,  8.3332e-01,\n",
            "          1.0333e+00,  5.4578e-01,  5.5361e-01,  8.1984e-01, -1.3906e-02,\n",
            "          6.3026e-01,  1.6284e-01,  4.2860e-01,  1.1859e-01, -3.0409e-01,\n",
            "          2.1727e-01,  4.3405e-01,  2.8170e-02,  7.3813e-01,  2.9306e-01,\n",
            "          6.2157e-01,  6.0909e-01, -7.5225e-01,  5.1779e-01,  7.1798e-01,\n",
            "         -5.6297e-01,  4.3861e-01,  2.2782e-01, -3.5523e-01,  6.7361e-02,\n",
            "         -4.3283e-01, -7.8890e-01, -2.4002e-01,  3.6656e-01,  6.0394e-01,\n",
            "          7.4094e-01,  3.6543e-01,  6.5204e-01, -4.9183e-02, -3.3282e-01,\n",
            "         -9.0207e-01, -1.0826e+00, -5.1010e-01,  7.7260e-01, -1.1998e+00,\n",
            "         -9.8556e-01, -1.0154e+00, -6.9063e-01, -1.2041e+00, -9.1302e-01,\n",
            "         -3.0337e-01,  6.9894e-01,  7.4682e-01, -2.3170e-01,  1.7817e-01,\n",
            "          8.3181e-01, -1.8078e-01,  3.6112e-03, -8.8868e-01, -1.7238e+00,\n",
            "         -9.8755e-01, -1.2085e+00, -1.4302e-01, -9.0542e-01, -1.0691e+00,\n",
            "         -1.1115e+00, -1.1715e+00, -1.3931e+00, -5.0668e-01, -1.3930e-01,\n",
            "         -1.7293e+00, -7.5069e-01, -3.4249e-01, -3.9131e-01, -1.0938e+00,\n",
            "         -8.2571e-01,  1.0520e-01, -8.0561e-01, -1.3639e+00, -5.0878e-01,\n",
            "          1.3908e-01, -2.9421e-01, -5.6506e-01,  2.6151e-01,  5.4645e-01,\n",
            "         -4.3854e-01, -9.3235e-01, -1.2386e+00, -1.0480e+00, -7.4863e-01,\n",
            "         -1.3958e+00, -1.2291e+00, -1.3664e+00, -1.2879e+00, -1.4558e+00,\n",
            "         -1.6012e+00, -1.2873e+00,  1.7558e-01, -8.9050e-03, -4.4856e-01,\n",
            "         -2.6422e-02, -3.2114e-01, -5.7442e-02, -2.8530e-01, -4.1419e-01,\n",
            "         -6.8383e-01, -1.5625e+00,  1.4072e-01,  5.6261e-01, -1.2385e+00,\n",
            "         -4.3581e-01,  3.1890e-01, -7.7564e-01, -1.7140e+00, -9.8095e-01,\n",
            "          5.4061e-01, -9.1628e-01, -1.8580e+00, -1.5921e-01, -1.3213e+00,\n",
            "         -1.2198e+00, -2.4066e+00, -1.5291e+00, -8.1029e-01, -7.6361e-01,\n",
            "          4.5303e-01,  1.2575e+00,  1.5141e-01,  7.1433e-01,  5.1864e-01,\n",
            "         -2.2698e-01,  8.9072e-01,  2.8976e-01,  4.7651e-01, -2.9222e-01,\n",
            "         -4.8500e-01, -1.0427e+00, -2.1050e-01, -5.7244e-01, -5.4037e-01,\n",
            "         -7.6108e-01, -5.8492e-02, -4.3288e-01,  7.4542e-02, -3.6825e-01,\n",
            "         -9.3779e-01, -8.3831e-01,  1.6894e-01, -1.9757e-01, -4.2266e-01,\n",
            "          3.2379e-01, -2.5826e-01, -2.8207e-02, -2.1307e-01, -4.4138e-01,\n",
            "         -6.8362e-01, -1.2469e+00, -7.1016e-01, -7.1413e-01,  3.2435e-02,\n",
            "          4.2862e-01,  1.6020e-01, -1.6225e+00, -1.9335e+00, -3.2140e-01,\n",
            "          6.5381e-01, -1.3067e+00, -9.2424e-01,  5.7795e-01, -2.6462e-01,\n",
            "         -8.5022e-01,  1.0528e+00,  3.2732e-01, -2.0447e+00, -1.8997e+00,\n",
            "         -4.5857e-01, -2.7197e-01, -4.6804e-01, -4.4100e-01,  1.1237e+00,\n",
            "         -3.5775e-01,  3.2747e-01,  1.9194e+00,  8.7152e-01,  4.9316e-01,\n",
            "          8.7261e-01,  4.0107e-02,  2.5687e-01,  1.2706e-01,  1.1470e+00,\n",
            "          6.6473e-01,  1.3130e+00,  4.3822e-02,  3.3206e-02, -7.0372e-02,\n",
            "         -1.0698e+00, -1.4885e-01,  1.2113e+00,  1.6841e+00,  6.5950e-01,\n",
            "         -5.3450e-01,  9.0312e-02,  1.6198e-01,  6.4224e-01,  6.1280e-01,\n",
            "          7.8700e-01, -2.3152e-01, -4.9247e-01,  2.5764e-01,  3.2653e-01,\n",
            "          9.1964e-01,  4.0120e-01,  1.1999e-01, -2.6706e-01,  1.2022e-01,\n",
            "          3.4970e-01, -6.2430e-02,  1.7013e+00,  1.1220e+00, -7.1037e-01,\n",
            "         -4.0156e-01,  2.4399e-01,  6.4349e-01, -1.9432e-01, -3.4572e-01,\n",
            "          3.7426e-01,  1.5479e+00,  1.1497e+00, -4.5670e-02,  6.8467e-01,\n",
            "         -8.4776e-01,  5.5485e-01,  1.5440e+00,  2.3499e+00,  9.7719e-01,\n",
            "         -3.3241e-01, -1.4951e+00,  1.3036e-01, -9.0984e-02,  1.5547e+00,\n",
            "          1.0045e+00,  6.6845e-01,  3.2643e-01,  9.9273e-01, -1.7001e-01,\n",
            "         -1.0965e-02,  6.8471e-01,  5.3754e-01,  8.7095e-01,  5.1154e-01,\n",
            "         -2.1204e-01,  3.6029e-02,  2.3052e-01, -9.3355e-01, -1.3991e+00,\n",
            "         -5.7257e-02, -2.6020e-01,  1.1763e+00,  1.7229e+00,  1.1315e+00,\n",
            "          5.5532e-01,  1.0924e+00,  6.1879e-01, -1.2698e+00,  1.4856e+00,\n",
            "         -1.2122e+00,  2.0134e-02, -3.1865e-01, -1.7393e-01,  1.0557e+00,\n",
            "         -1.5853e+00,  5.0536e-01,  1.1922e+00,  1.5273e-01,  8.1168e-01,\n",
            "          9.4818e-01,  1.1923e+00,  6.0291e-01,  4.1156e-01,  3.7784e-01,\n",
            "         -1.3105e+00, -5.7190e-01,  8.1455e-01,  3.6329e-01,  1.0479e+00,\n",
            "          1.8084e+00,  3.4183e-01, -5.1544e-02,  1.6784e+00,  8.2130e-01,\n",
            "         -5.9277e-01,  6.2437e-01,  8.3530e-01,  1.9372e+00,  3.4593e-01,\n",
            "         -1.2037e+00, -6.5947e-02, -5.2134e-01,  2.9200e-01, -7.1726e-02,\n",
            "          8.8661e-01,  1.6225e-01,  5.7711e-02, -9.0770e-01,  4.8293e-02,\n",
            "         -4.7611e-01, -4.6642e-01, -5.3581e-01,  8.0294e-02,  1.3230e+00,\n",
            "         -1.0782e+00,  1.7633e+00,  1.1502e+00,  7.8919e-01,  4.6963e-01,\n",
            "          7.9016e-01,  3.9902e-01, -2.0654e+00, -1.6173e+00, -3.3302e-01,\n",
            "         -4.7008e-01,  1.0165e-01,  6.8167e-01, -1.9866e-01, -1.7476e+00,\n",
            "         -6.1555e-01,  1.8513e-01,  4.3866e-01,  1.3239e+00,  7.8624e-01,\n",
            "          1.5124e-01, -3.2972e-01,  8.1333e-01, -4.6708e-02, -1.2708e+00,\n",
            "         -9.3539e-01,  3.2102e-01,  1.0624e+00,  5.9486e-01, -5.5511e-01,\n",
            "          1.0672e+00, -5.0773e-03,  7.3097e-01, -6.2004e-01,  5.5226e-01,\n",
            "          6.6500e-02, -8.6359e-01,  1.0295e+00, -4.6820e-02,  2.4487e-01,\n",
            "          4.5267e-01, -5.5491e-02,  8.0872e-01,  6.4470e-01,  9.4575e-01,\n",
            "          6.9576e-01, -1.7639e-01,  1.8663e+00,  1.1002e+00,  1.4019e+00,\n",
            "         -5.1494e-01,  3.9839e-01, -7.0262e-01,  7.2921e-01,  7.5762e-02,\n",
            "         -5.6651e-01,  1.1053e+00, -3.5364e-01, -7.8166e-01,  8.6980e-01,\n",
            "          2.0951e+00, -1.0550e-01,  1.5226e-01, -3.2120e-01,  3.0791e-01,\n",
            "         -3.8749e-02,  1.4323e+00, -4.3007e-01,  4.6377e-01, -9.4166e-03,\n",
            "          8.1119e-01,  9.9567e-01, -2.9354e-01,  6.4799e-01,  1.0114e-01,\n",
            "          1.7553e-01,  1.1170e+00,  4.3455e-01,  1.6823e+00,  8.8279e-01,\n",
            "          9.6897e-01,  9.6373e-01,  1.0397e-01,  1.7747e-01,  1.4125e-01,\n",
            "         -1.0727e+00,  9.1098e-01, -3.9007e-01, -1.1205e+00,  2.1179e-01,\n",
            "         -2.9096e-01,  7.2548e-01,  6.6779e-01,  1.0383e+00, -7.3472e-02,\n",
            "          1.2872e-01,  1.2244e+00,  6.6346e-01,  7.2897e-01,  9.1039e-02,\n",
            "         -1.4449e+00,  1.2654e+00,  5.9485e-02,  1.4545e+00,  7.6316e-01,\n",
            "         -4.4986e-01,  8.2201e-01,  3.6550e-01, -1.0118e+00, -1.3498e+00,\n",
            "          1.1161e+00, -2.8589e-01,  9.1583e-01,  7.8986e-01, -1.2339e-01,\n",
            "          7.6752e-01,  7.5919e-02,  1.4639e-01,  2.3700e-01,  2.4036e-01,\n",
            "          7.7075e-02, -8.2844e-01, -2.0413e-01, -9.2788e-01,  1.0246e+00,\n",
            "          2.5291e-01,  1.1670e+00,  3.1154e-01, -7.9443e-01, -3.9162e-01,\n",
            "          4.0369e-01, -1.9996e-02, -4.7707e-01,  6.7280e-01,  1.2169e+00,\n",
            "         -8.4327e-01,  1.5573e+00,  1.0845e+00,  1.0930e+00, -1.0621e-02,\n",
            "          3.9202e-01,  3.9974e-01, -6.2718e-01,  5.3807e-01,  8.7113e-01,\n",
            "         -1.1883e+00,  3.7843e-02, -1.0312e+00, -3.6074e-01, -9.3771e-01,\n",
            "         -5.8914e-01,  6.6014e-01,  8.3357e-01,  7.6969e-01, -1.1759e+00,\n",
            "          9.4440e-01,  1.4558e+00,  1.6510e-01, -4.0689e-01,  1.0505e+00,\n",
            "          1.7758e+00, -2.6241e-01, -2.0976e-02,  7.4559e-01,  7.8716e-01,\n",
            "         -3.0994e-01, -3.1620e-01,  2.9821e-01,  8.2470e-01,  4.2390e-03,\n",
            "          1.0010e+00,  7.3792e-01, -9.9941e-02, -6.7521e-01,  7.8275e-02,\n",
            "         -2.0788e-01,  8.4673e-01, -5.6203e-01, -2.4159e-01,  4.6365e-01,\n",
            "         -1.0658e-01,  3.3845e-02,  1.4130e+00,  2.3971e-01, -5.1029e-01,\n",
            "          1.0223e+00, -9.7087e-01, -1.7414e-01,  1.3144e+00, -6.9537e-01,\n",
            "          1.9025e-02,  2.0917e+00, -7.2098e-01,  2.2526e+00, -1.5084e+00,\n",
            "          3.8549e-01, -2.4483e-01,  4.4335e-01,  1.0674e+00,  1.8378e-01,\n",
            "          1.2165e+00,  3.1212e-02,  2.8837e-01,  3.1066e-01,  7.4413e-01,\n",
            "          2.5955e-01, -1.6699e-02,  3.6627e-01,  6.4960e-01,  1.2741e+00,\n",
            "          5.2557e-01, -3.8533e-02,  3.2071e-01,  6.1192e-01,  9.4732e-01,\n",
            "         -5.3056e-01,  7.6891e-01, -4.8915e-02,  1.4642e+00, -1.9013e-01,\n",
            "         -1.7550e-01,  6.1736e-01,  7.3168e-01,  7.7482e-01,  1.4197e+00,\n",
            "          1.0914e+00,  4.9070e-01,  4.0210e-01,  2.1690e-01,  1.1661e+00,\n",
            "          5.1115e-01,  4.0136e-01,  1.4789e+00,  3.7395e-01,  9.3137e-01,\n",
            "          8.1727e-02,  3.9513e-01,  5.0852e-01,  1.2603e+00, -7.5929e-01,\n",
            "         -9.2312e-01, -5.1447e-01,  9.8695e-01,  9.5125e-01,  1.6806e+00,\n",
            "          4.4041e-01,  7.1258e-01,  1.4082e+00,  4.2693e-01, -1.9268e-01,\n",
            "          7.8009e-01,  1.5010e+00,  1.5189e+00,  1.0108e+00, -3.0351e-03,\n",
            "          6.0705e-02,  1.0943e+00,  6.9926e-01, -9.2727e-01,  7.6793e-02,\n",
            "         -6.7871e-01,  1.8515e-01, -1.1660e+00, -1.6121e+00,  7.7471e-01,\n",
            "          1.3239e+00,  2.8566e-01,  2.8919e-01,  1.5093e+00,  3.8541e-02,\n",
            "         -7.3263e-01,  1.0947e+00, -4.4803e-01,  1.5855e+00, -1.1570e+00,\n",
            "         -4.4747e-01,  4.5428e-01, -9.7046e-01,  1.7032e+00,  3.6863e-01,\n",
            "         -1.5904e+00, -1.0495e+00,  2.7277e-01,  6.5307e-01,  1.1955e+00,\n",
            "         -6.6212e-01,  2.8615e-01,  7.5061e-01,  1.3001e+00, -7.1736e-01,\n",
            "          1.1707e+00,  3.6929e-01, -8.2873e-01, -9.8543e-01, -5.5052e-02,\n",
            "          5.1786e-01,  1.6711e+00,  1.7795e+00,  1.0256e+00, -5.9183e-01,\n",
            "          1.4250e+00,  5.0974e-01,  2.6252e-01,  7.1634e-01,  3.2432e-01,\n",
            "          1.3954e+00,  8.1963e-01, -2.9724e-01,  3.8397e-01,  7.9494e-01,\n",
            "          1.4439e+00,  1.2138e+00,  1.9184e+00, -4.3959e-01, -5.8756e-01,\n",
            "          9.1018e-01, -2.6894e-01, -1.7076e-01, -1.4901e-01,  9.7200e-01,\n",
            "          8.7824e-02,  1.4475e+00,  6.6473e-01,  2.1649e-01, -1.6482e-01,\n",
            "          8.0455e-01,  2.2759e-01, -3.7101e-01,  1.3111e+00, -7.6450e-01,\n",
            "          1.6106e+00, -1.4288e+00,  1.2407e+00, -1.2306e+00, -2.5138e+00,\n",
            "          4.6272e-01,  1.4241e+00, -1.6117e-01, -3.1599e-01,  1.5240e+00,\n",
            "          1.5476e+00, -3.2091e-01,  1.1853e+00,  1.2838e+00, -8.4366e-02,\n",
            "          1.9310e-01, -6.8061e-01, -3.6083e-01, -1.3472e+00,  3.4449e-01,\n",
            "         -5.6220e-01,  1.1023e-01,  7.0376e-01, -2.3685e-01, -1.0205e+00,\n",
            "         -4.7839e-01,  8.5010e-01,  5.5244e-01,  1.7219e+00,  1.8980e+00,\n",
            "         -9.3734e-01, -3.7327e-01,  1.6919e+00,  8.0093e-01,  8.5271e-01,\n",
            "          9.5288e-02, -6.2877e-01,  1.3032e+00, -1.0176e+00,  8.5021e-01,\n",
            "          1.2304e+00,  1.2737e+00,  9.1091e-01, -6.4732e-01, -2.2763e+00,\n",
            "         -4.5326e-01,  7.7039e-02,  3.6984e-01,  4.5280e-01,  8.8465e-02,\n",
            "         -1.3420e-01,  9.5575e-01, -7.6686e-01,  4.9859e-01, -1.8673e-01,\n",
            "         -9.4098e-01, -7.9845e-01, -6.3765e-01, -2.6179e-01,  1.5543e+00,\n",
            "         -2.8174e-01, -2.4847e-01,  6.0168e-01, -1.7912e+00, -5.8944e-02,\n",
            "         -5.7940e-01,  5.4022e-01,  1.7424e-01,  1.9193e-01, -1.9340e-01,\n",
            "         -4.8661e-01, -7.6169e-01, -2.0389e-01,  2.3943e-01, -1.9203e-01,\n",
            "         -8.2991e-01, -8.2108e-01,  5.9524e-01,  7.2015e-01, -3.5981e-01,\n",
            "          5.0129e-02, -7.1901e-01, -5.3946e-01,  1.8408e-01,  6.3383e-01,\n",
            "         -3.9973e-01, -2.4814e-01, -6.6561e-01,  1.8243e-01, -9.4168e-01,\n",
            "          3.3566e-01,  6.9614e-02, -3.9057e-01, -7.5447e-01, -1.3404e+00,\n",
            "         -1.4101e-01,  5.0262e-01, -8.2815e-01,  5.8975e-01, -5.2299e-02,\n",
            "          2.4730e-03,  1.0633e+00, -4.7458e-01, -3.1615e-01, -2.2661e+00,\n",
            "          9.4599e-01, -1.6278e+00,  2.1839e-01,  1.1331e-01, -8.9232e-01,\n",
            "         -1.0060e+00,  1.1250e-01,  5.8927e-02, -4.5406e-01, -9.6432e-01,\n",
            "         -1.5334e+00, -2.2921e+00,  1.7387e+00, -1.4887e-01, -8.7834e-01,\n",
            "         -2.9636e-01, -1.0227e+00, -8.2967e-01, -1.6448e+00, -5.6819e-01,\n",
            "         -1.7962e-01,  4.3062e-01, -2.1499e-01,  1.8691e+00,  8.1297e-01]],\n",
            "       grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们使用模型的预测值和相应的标签来计算误差（loss）。下一步是沿着网络反向传播这个误差。当我们对误差张量调用 `.backward()` 时，反向传播就开始了。Autograd 随后会计算每个模型参数的梯度，并将其存储在参数的 `.grad` 属性中。"
      ],
      "metadata": {
        "id": "rn4m26OgKbUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = (prediction - labels).sum() # 计算损失\n",
        "loss.backward() # 反向传播"
      ],
      "metadata": {
        "id": "UVMuwo5MKjd3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "接着，我们加载一个优化器，这里我们使用随机梯度下降（SGD），学习率为 0.01，动量为 0.9。我们将模型的所有参数注册到优化器中。"
      ],
      "metadata": {
        "id": "d4gxiqp6LfYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9) # 创建优化器"
      ],
      "metadata": {
        "id": "NBAapA9zLjxS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "最后，我们调用 `.step()` 来启动梯度下降。优化器会根据存储在 `.grad` 中的梯度来调整每个参数。"
      ],
      "metadata": {
        "id": "OA7RX-NxL217"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optim.step() # 梯度下降"
      ],
      "metadata": {
        "id": "mZC4MbouL3qa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "至此，你已经掌握了训练神经网络所需的一切。以下各节将详细介绍 autograd 的工作原理——如果你不感兴趣，可以跳过它们。"
      ],
      "metadata": {
        "id": "WGSaw-EVMM_S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Autograd 中的微分\n",
        "我们来看看 autograd 是如何收集梯度的。我们创建两个张量 `a` 和 `b`，并设置 `requires_grad=True`。这会向 autograd 发出信号，表明对它们的所有操作都应被追踪。"
      ],
      "metadata": {
        "id": "mz0AK-zjMgna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "a = torch.tensor([2., 3.], requires_grad=True)\n",
        "b = torch.tensor([6., 4.], requires_grad=True)"
      ],
      "metadata": {
        "id": "DEcnYtA5NqxC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们用 `a` 和 `b` 创建另一个张量 `Q`。\n",
        "$$\n",
        "Q = 3a^3 - b^2\n",
        "$$"
      ],
      "metadata": {
        "id": "eu8iws8_N7F5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Q = 3*a**3 - b**2"
      ],
      "metadata": {
        "id": "CvPnKyIpOIDy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "假设 `a` 和 `b` 是一个神经网络的参数，而 `Q` 是误差。在神经网络训练中，我们想要的是误差关于参数的梯度，即：\n",
        "$$\n",
        "\\frac{\\partial Q}{\\partial a} = 9a^2\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial Q}{\\partial b} = -2b\n",
        "$$\n",
        "当我们对 `Q` 调用 `.backward()` 时，autograd 会计算这些梯度并将它们存储在对应张量的 `.grad` 属性中。\n",
        "\n",
        "我们需要在 `Q.backward()` 中显式地传递一个 `gradient` 参数，因为 `Q` 是一个向量。`gradient` 是一个与 `Q` 形状相同的张量，它代表 `Q` 相对于其自身的梯度，即：\n",
        "$$\n",
        "\\frac{dQ}{dQ} = 1\n",
        "$$\n",
        "等价地，我们也可以将 `Q` 聚合成一个标量并隐式地调用 backward，例如 `Q.sum().backward()`。"
      ],
      "metadata": {
        "id": "3rf0wHkJORMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "external_grad = torch.tensor([1., 1.])\n",
        "Q.backward(gradient=external_grad) #计算梯度"
      ],
      "metadata": {
        "id": "K7b1Y13hOSlZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "现在梯度已经被存储在 `a.grad` 和 `b.grad` 中了。"
      ],
      "metadata": {
        "id": "m0NzWq8XUgoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 检查收集的梯度是否正确\n",
        "print(9*a**2 == a.grad)\n",
        "print(-2*b == b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymbyEY-uUlje",
        "outputId": "63064b3c-e3e4-4c6e-d4f8-d2c123433e91"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([True, True])\n",
            "tensor([True, True])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 选读 - 使用 autograd 进行向量微积分\n",
        "从数学上讲，如果你有一个向量值函数 $\\vec{y} = f(\\vec{x})$，那么 $\\vec{y}$ 关于 $\\vec{x}$ 的梯度是一个雅可比矩阵 $J$:\n",
        "$$\n",
        "J = \\begin{pmatrix} \\frac{\\partial y}{\\partial x_1} & ... & \\frac{\\partial y}{\\partial x_n} \\end{pmatrix} = \\begin{pmatrix} \\frac{\\partial y_1}{\\partial x_1} & \\cdots & \\frac{\\partial y_1}{\\partial x_n} \\\\ \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial y_m}{\\partial x_1} & \\cdots & \\frac{\\partial y_m}{\\partial x_n} \\end{pmatrix}\n",
        "$$\n",
        "总的来说，`torch.autograd` 是一个用于计算**向量-雅可比积**的引擎。也就是说，给定任何向量 $\\vec{v}$，计算乘积 $J^T \\cdot \\vec{v}$。\n",
        "\n",
        "如果 $\\vec{v}$ 恰好是一个标量函数 $l=g(\\vec{y})$ 的梯度：\n",
        "$$\n",
        "\\vec{v} = \\begin{pmatrix} \\frac{\\partial l}{\\partial y_1} & \\cdots & \\frac{\\partial l}{\\partial y_m} \\end{pmatrix}^T\n",
        "$$\n",
        "那么根据链式法则，向量-雅可比积就是 $l$ 关于 $\\vec{x}$ 的梯度：\n",
        "$$\n",
        "J^T \\cdot \\vec{v} = \\begin{pmatrix} \\frac{\\partial y_1}{\\partial x_1} & \\cdots & \\frac{\\partial y_m}{\\partial x_1} \\\\ \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial y_1}{\\partial x_n} & \\cdots & \\frac{\\partial y_m}{\\partial x_n} \\end{pmatrix} \\begin{pmatrix} \\frac{\\partial l}{\\partial y_1} \\\\ \\vdots \\\\ \\frac{\\partial l}{\\partial y_m} \\end{pmatrix} = \\begin{pmatrix} \\frac{\\partial l}{\\partial x_1} \\\\ \\vdots \\\\ \\frac{\\partial l}{\\partial x_n} \\end{pmatrix}\n",
        "$$\n",
        "向量-雅可比积的这个特性正是我们在上一个例子中使用的；`external_grad` 就代表了向量 $\\vec{v}$。"
      ],
      "metadata": {
        "id": "rfOjHKt3LxRe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 计算图 (Computational Graph)\n",
        "从概念上讲，autograd 在一个由 `Function` 对象组成的**有向无环图 (DAG)** 中记录了数据（张量）和所有执行的操作（以及由此产生的新张量）。在这个 DAG 中，叶节点是输入张量，根节点是输出张量。通过从根到叶追溯这个图，你可以使用链式法则自动计算梯度。\n",
        "\n",
        "在前向传播中，autograd 同时做两件事：\n",
        "*   运行请求的操作以计算结果张量。\n",
        "*   在 DAG 中维护该操作的梯度函数。\n",
        "\n",
        "当在 DAG 的根节点上调用 `.backward()` 时，反向传播就开始了。`autograd` 接着会：\n",
        "*   从每个 `.grad_fn` 计算梯度。\n",
        "*   将它们累加到相应张量的 `.grad` 属性中。\n",
        "*   使用链式法则，一直传播到叶节点张量。\n",
        "\n",
        "下面是我们的例子中 DAG 的一个可视化表示。在图中，箭头指向前向传播的方向。节点代表前向传播中每个操作的反向函数。蓝色的叶节点代表我们的叶张量 `a` 和 `b`。\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "> **注意**\n",
        ">\n",
        "> **PyTorch 中的 DAG 是动态的**\n",
        "> 一个重点是，图是从头开始重新创建的；在每次 `.backward()` 调用之后，autograd 都会开始构建一个新的图。这正是允许你在模型中使用控制流语句的原因；如果需要，你可以在每次迭代中改变形状、大小和操作。"
      ],
      "metadata": {
        "id": "nwJyzqomL7zG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 从 DAG 中排除\n",
        "`torch.autograd` 会追踪所有 `requires_grad` 标志为 `True` 的张量上的操作。对于不需要梯度的张量，将其 `requires_grad` 属性设置为 `False` 会将其从梯度计算的 DAG 中排除。\n",
        "\n",
        "即使只有一个输入张量设置了 `requires_grad=True`，操作的输出张量也将需要梯度。"
      ],
      "metadata": {
        "id": "8ZJDCZ8SMkSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(5, 5) # 创建一个5x5的张量\n",
        "y = torch.rand(5, 5) # 创建一个5x5的张量\n",
        "z = torch.rand((5, 5), requires_grad=True) # 创建一个5x5的张量，并设置requires_grad=True\n",
        "\n",
        "a = x + y # 计算x和y的和\n",
        "print(f\"a 是否需要梯度?: {a.requires_grad}\") # 打印a是否需要梯度\n",
        "b = x + z # 计算x和z的和\n",
        "print(f\"b 是否需要梯度?: {b.requires_grad}\") # 打印b是否需要梯度"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4pTO5l9MPwV",
        "outputId": "886d2427-54b0-449e-e122-040b6a12310f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a 是否需要梯度?: False\n",
            "b 是否需要梯度?: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "在神经网络中，不计算梯度的参数通常被称为**冻结参数** (frozen parameters)。如果你预先知道你不需要某些参数的梯度，那么“冻结”模型的一部分会很有用（这通过减少 autograd 的计算来提供一些性能优势）。\n",
        "\n",
        "在微调（finetuning）中，我们冻结模型的大部分，通常只修改分类器层来对新标签进行预测。让我们通过一个小例子来演示这一点。和之前一样，我们加载一个预训练的 resnet18 模型，并冻结所有参数。"
      ],
      "metadata": {
        "id": "VtLGNFo-PS0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn, optim\n",
        "\n",
        "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
        "\n",
        "# 冻结网络中的所有参数\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "bWR7ReMpTss6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "假设我们想在一个有 10 个标签的新数据集上微调模型。在 resnet 中，分类器是最后一个线性层 `model.fc`。我们可以简单地用一个新的线性层（默认情况下是未冻结的）替换它，作为我们的分类器。"
      ],
      "metadata": {
        "id": "WtF7uRxTUcHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fc = nn.Linear(512, 10)  # 将模型的最后一层替换为一个新的全连接层"
      ],
      "metadata": {
        "id": "4ERX3ZTZUdIY"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "现在，除了 `model.fc` 的参数外，模型中的所有参数都被冻结了。唯一计算梯度的参数是 `model.fc` 的权重和偏置。"
      ],
      "metadata": {
        "id": "aQ3N386wfqcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 只优化分类器\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
      ],
      "metadata": {
        "id": "QWUXQSo0f0oC"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "注意，尽管我们在优化器中注册了所有的参数，但唯一计算梯度（并因此在梯度下降中更新）的参数是分类器的权重和偏置。\n",
        "\n",
        "同样，`torch.no_grad()` 上下文管理器也提供了这种排除计算的功能。"
      ],
      "metadata": {
        "id": "Li6XprYqgjZC"
      }
    }
  ]
}